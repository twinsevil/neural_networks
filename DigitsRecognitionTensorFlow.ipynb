{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Распознавание рукописных символов на TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-755dee0a5ffb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtutorials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minput_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmnist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_data_sets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"MNIST/\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\examples\\tutorials\\mnist\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtutorials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minput_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtutorials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\examples\\tutorials\\mnist\\input_data.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mxrange\u001b[0m  \u001b[1;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mread_data_sets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;31m# pylint: enable=unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\contrib\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcopy_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcrf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcudnn_rnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\contrib\\cudnn_rnn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;31m# pylint: disable=unused-import,wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn_rnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;31m# pylint: enable=unused-import,wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\contrib\\cudnn_rnn\\python\\layers\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# pylint: disable=unused-import,wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn_rnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn_rnn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;31m# pylint: enable=unused-import,wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\contrib\\cudnn_rnn\\python\\layers\\cudnn_rnn.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn_rnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcudnn_rnn_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\contrib\\cudnn_rnn\\python\\ops\\cudnn_rnn_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msplit_dependency\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlstm_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\contrib\\rnn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfused_rnn_cell\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgru_ops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm_ops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn_cell\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\contrib\\rnn\\python\\ops\\lstm_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m _lstm_ops_so = loader.load_op_library(\n\u001b[1;32m---> 35\u001b[1;33m     resource_loader.get_path_to_datafile(\"_lstm_ops.so\"))\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[0mLayerRNNCell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnn_cell_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLayerRNNCell\u001b[0m  \u001b[1;31m# pylint: disable=invalid-name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\contrib\\util\\loader.py\u001b[0m in \u001b[0;36mload_op_library\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;31m# To avoid making every user_ops aware of windows, re-write\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;31m# the file extension from .so to .dll if .so file doesn't exist.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m       \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'\\.so$'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'.dll'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf-gpu\\lib\\genericpath.py\u001b[0m in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;34m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "mnist = input_data.read_data_sets(\"MNIST/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax-регрессия (нейронная сеть без скрытых слоев)\n",
    "\n",
    "В качестве модели для обучения рассмотрим softmax-регрессию. Это обобщение логистической регресии на случай нескольких классов: чтобы получить \"вероятности\"  классов применяем softmax-функцию к вектору получившихся ненормализованных оценок:\n",
    "\n",
    "$$softmax(x)_j = \\frac{\\exp{(x_j)}}{\\sum_i exp{(x_i)}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784], name=\"x1\") \n",
    "W = tf.Variable(tf.zeros([784, 10]), name=\"W1\")\n",
    "b = tf.Variable(tf.zeros([10]), name=\"b1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "y_ = tf.placeholder(tf.float32, [None, 10], name=\"y_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве функции потерь используем стандартную для логистической регрессии перекрестную энтропию (cross-entropy):\n",
    "\n",
    "$$H_t(y) = - \\sum_i t_i \\log (y_i),$$\n",
    "\n",
    "где $y$ -- предсказанное значение, а $t$ -- исходная разметка (правильный ответ).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "cost_list = []\n",
    "accuracy_list = []\n",
    "n = 1000\n",
    "for i in range(n):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    _, cost, accur = sess.run([train_step, cross_entropy, accuracy],\n",
    "                              feed_dict={x: batch_xs, y_: batch_ys})    \n",
    "    cost_list.append(cost)\n",
    "    accuracy_list.append(accur)\n",
    "     \n",
    "ax1.plot(range(0, n), cost_list)\n",
    "ax1.set_title(\"Cost function\")\n",
    "ax1.set_ylim(0, 2)\n",
    "ax2.plot(range(0, n), accuracy_list, c='r')\n",
    "ax2.set_title(\"Accuracy\")\n",
    "ax2.set_ylim(0, 1)\n",
    "#plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: %s\" % sess.run(accuracy, feed_dict = {x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, точность предсказания на тестовом множестве для модели softmax-регрессии получилась приблизительно равна 91%. Попробуем улучшить точность с помощью более сложных моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Визуализируем цифры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем предсказание на тестовом множестве и посмотрим на несколько цифр из тестовой выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_images = mnist.test.images\n",
    "test_images_labels = np.argmax(mnist.test.labels, axis=1)\n",
    "pred = tf.argmax(y, 1)\n",
    "pred = sess.run(pred, feed_dict = {x: mnist.test.images})\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.size'] = 20   \n",
    "def show_digits(images_array, image_labels, predictions, indices = range(5)):\n",
    "    columns = 5\n",
    "    rows = len(indices)//columns + (0 if len(indices)%columns == 0 else 1)        \n",
    "    fig = plt.figure(figsize=(25, 25))        \n",
    "    for i, ind in enumerate(indices):    \n",
    "        ax = fig.add_subplot(rows, columns, i+1)      \n",
    "        image = np.reshape(images_array[ind], [28, 28])\n",
    "        str_title = \"Label:\" + str(image_labels[ind]) + \" Prediction:\" + str(predictions[ind])\n",
    "        ax.set_title(str_title)\n",
    "        plt.imshow(image)    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_digits(test_images, test_images_labels, pred, range(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдем индексы изображений, которые были классифицированы ошибочно, и посмотрим как выглядели исходные данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mistakes_indices = test_images_labels != pred\n",
    "print(\"Proportion of errors:\", np.mean(mistakes_indices), \"Accuracy:\",  np.mean(np.logical_not(mistakes_indices)))\n",
    "indices_of_wrong_predictions = np.array(range(len(test_images_labels)))[mistakes_indices]   \n",
    "print(\"Total wrong predictions:\", len(indices_of_wrong_predictions))\n",
    "show_digits(test_images, test_images_labels, pred, indices_of_wrong_predictions[ :15] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отметим для себя, что некоторые цифры представляют сложность в распознавании даже для человека и двинемся дальше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нейронная сеть с одним скрытым ReLU-слоем и дропаутом\n",
    "\n",
    "Будем использовать 100 нейронов на скрытом слое."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_relu = tf.Variable(tf.truncated_normal([784, 100], stddev=0.1), name=\"W_relu\")\n",
    "b_relu = tf.Variable(tf.truncated_normal([100], stddev=0.1), name=\"b_relu\")\n",
    "h = tf.nn.relu(tf.matmul(x, W_relu) + b_relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будет полезен слой дропаута. Дропаут -- это слой, который выбрасывает (обнуляет) выходы некоторых нейронов, выбираемых случайно и заново для каждого обучающего примера. Все, что нужно сейчас задать -- это вероятность их выбрасывания. для этого сначала создадим заглушку:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_probability = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "h_drop = tf.nn.dropout(h, keep_probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нейроны скрытого слоя будут участвовать в вычислениях с вероятностью `keep_propability`, а с вероятностью `1-keep_probability` их выход будет обнулен, и они не будут ни участвовать в предсказании для этого примера, ни обучаться на нем. \n",
    "Поскольку размер внутреннего слоя отличается от входного, придется немного поменять параметры внешнего слоя и, кроме того, переписать заключительный softmax-слой:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([100, 10]), name=\"W2\")\n",
    "b = tf.Variable(tf.zeros([10]), name=\"b2\")\n",
    "y = tf.nn.softmax(tf.matmul(h_drop, W) + b)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "cost_list = []\n",
    "accuracy_list = []\n",
    "n = 2000\n",
    "for i in range(n):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    _, cost, accur = sess.run([train_step, cross_entropy, accuracy],\n",
    "                              feed_dict={x: batch_xs, y_: batch_ys, keep_probability: 0.5})    \n",
    "    #accur = sess.run(accuracy, feed_dict={x:mnist.test.images, y_:mnist.test.labels, keep_probability:1.})\n",
    "    cost_list.append(cost)\n",
    "    accuracy_list.append(accur)\n",
    "mpl.rcParams['font.size'] = 20    \n",
    "ax1.plot(range(0, n), cost_list)\n",
    "ax1.set_title(\"Cost function\")\n",
    "ax2.plot(range(0, n), accuracy_list, c='r')\n",
    "\n",
    "ax2.set_title(\"Accuracy\")\n",
    "ax2.set_ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy:{0:.4f}\".format(sess.run(accuracy, feed_dict={x:mnist.test.images, y_:mnist.test.labels, keep_probability:1.})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что результат улучшился с добавлением скрытого ReLU слоя с дропаутом. Попробуем увеличить число шагов до 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "cost_list = []\n",
    "accuracy_list = []\n",
    "n = 3000\n",
    "for i in range(n):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    _, cost, accur = sess.run([train_step, cross_entropy, accuracy],\n",
    "                              feed_dict={x: batch_xs, y_: batch_ys, keep_probability: 0.5})    \n",
    "    #accur = sess.run(accuracy, feed_dict={x:mnist.test.images, y_:mnist.test.labels, keep_probability:1.})\n",
    "    cost_list.append(cost)\n",
    "    accuracy_list.append(accur)\n",
    " \n",
    "ax1.plot(range(0, n), cost_list)\n",
    "ax1.set_title(\"Cost function\")\n",
    "ax2.plot(range(0, n), accuracy_list, c='r')\n",
    "\n",
    "ax2.set_title(\"Accuracy\")\n",
    "ax2.set_ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хм. Видим, что примерно с 1500-й итерации точность прогноза упала до 10%! Подозрительно похоже на случайное угадывание равномерно распределенных 10 классов. В чем может быть причина такого поведения? Первое что нужно проверить - не сломалась ли наша модель. Для этого можно, например, посмотреть на ее веса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(sess.run(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sess.run(tf.argmax(y, axis=1), feed_dict={x:mnist.test.images, keep_probability:1.})) \n",
    "print(sess.run(cross_entropy, feed_dict={x:mnist.test.images, y_:mnist.test.labels, keep_probability:1.})) \n",
    "print(\"Accuracy:{0:.4f}\".format(sess.run(accuracy, feed_dict={x:mnist.test.images, y_:mnist.test.labels, keep_probability:1.})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Действительно, элементы вектора сводобных членов $b$ перестали быть числами. Чаще всего это говорит о переполнении. Оно происходит, когда числа выходят за пределы соответствующих диапазонов и округляются до $\\infty$. Чаще всего получается, когда очень маленькие числа округляются до нуля, а потом на этот ноль что-то делят. Но откуда переполнение возникло у нас?\n",
    "У нас имеется два места, где могла возникнуть подобная ошибка - функция softmax и функция потерь. \n",
    "\n",
    "К счастью, в TensorFlow позаботились об этих трудностях и подготовили функцию, которая обходит все подвожные камни. Это функция `softmax_cross_entropy_with_logits_v2`. Воспользуемся ей и посмотрим улучшится ли точность предсказаний."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_relu = tf.Variable(tf.truncated_normal([784, 100], stddev=0.1), name=\"W_relu3\")\n",
    "b_relu = tf.Variable(tf.truncated_normal([100], stddev=0.1), name=\"b_relu3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_drop = tf.nn.dropout(h, keep_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([100, 10]), name=\"W3\")\n",
    "b = tf.Variable(tf.zeros([10]), name=\"b3\")\n",
    "logit = tf.matmul(h_drop, W) + b\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=logit))\n",
    "y = tf.nn.softmax(tf.matmul(h_drop, W) + b)\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "cost_list = []\n",
    "accuracy_list = []\n",
    "n = 2000\n",
    "for i in range(n):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    _, cost, accur = sess.run([train_step, cross_entropy, accuracy],\n",
    "                              feed_dict={x: batch_xs, y_: batch_ys, keep_probability: 0.5})         \n",
    "    cost_list.append(cost)\n",
    "    accuracy_list.append(accur)\n",
    "\n",
    "ax1.plot(range(0, n), cost_list)\n",
    "ax1.set_title(\"Cost function\")\n",
    "\n",
    "ax2.plot(range(0, n), accuracy_list, c='r')\n",
    "ax2.set_title(\"Accuracy\")\n",
    "ax2.set_ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy:{0:.4f}\".format(sess.run(accuracy, feed_dict={x:mnist.test.images, y_:mnist.test.labels, keep_probability:1.})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Неплохо. Точность возрасла до 97%!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Как инициализировать веса. Инициализации Ксавье и Хе\n",
    "\n",
    "Результат сходимости градиентного спуска очень сильно зависит от начальных данных. Для симметричных функций активации с нулевым средним (в основном `tanh`) используйте инициализацию Ксавье, а для ReLU и ему подобных -- инициализацию Хе (`he_uniform`, `he_norm`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "from keras.utils import np_utils\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "X_train = x_train.reshape([-1, 28*28])/255\n",
    "X_test = x_test.reshape([-1, 28*28])/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "num_example = 3\n",
    "ax1.set_title(y_train[num_example])\n",
    "plt.imshow(x_train[num_example])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(init):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_shape=(28*28, ), kernel_initializer=init, activation='tanh'))\n",
    "    model.add(Dense(100, kernel_initializer=init, activation='tanh'))\n",
    "    model.add(Dense(100, kernel_initializer=init, activation='tanh'))\n",
    "    model.add(Dense(100, kernel_initializer=init, activation='tanh'))\n",
    "    model.add(Dense(10, kernel_initializer=init, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform_model = create_model(\"uniform\")\n",
    "uniform_model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "uniform_output = uniform_model.fit(X_train, Y_train, batch_size=64, epochs=30, verbose=1, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glorot_model = create_model(\"glorot_normal\")\n",
    "glorot_model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "glorot_output = glorot_model.fit(X_train, Y_train, batch_size=64, epochs=30, verbose=1, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(uniform_output.epoch, uniform_output.history['val_acc'])\n",
    "plt.plot(glorot_output.epoch, glorot_output.history['val_acc'])\n",
    "plt.legend([\"uniform\", \"glorot\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из графика делаем вывод, что при инициализации Ксавье, точность предсказания на первой же итерации гораздо выше, чем при инициализации весов с помощью равномерного распределения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нормализация по мини-батчам\n",
    "\n",
    "Из-за большого числа слоев возникает проблема *внутреннего сдвига переменных* (internal covariance shift). Т.е. ситуация, когда все чему обучался нейрон текущего слоя, становится бесполезным из-за изменения в распределении значений нейронов предыдущего слоя. Для решения этой проблемы используют нормализацию по мини-батчам.\n",
    "\n",
    "Однако нужно учесть тонкий момент. Если в качестве функции активации используется сигмоидальная функция $\\sigma(x) = \\frac{1}{1+e^{-x}}$, то когда мы нормализуем ее аргумент, мы увидим, что нелинейность почти пропадет: большинство нормализованных значений будут попадать в область, где сигмоида ведет себя как линейная функция. \n",
    "\n",
    "Чтобы компенсировать эти недостатки, слой нормализации должен быть способен работать как тождественная функция (т.е. при некоторых комбинациях параметров он должен иметь возможность со входами буквально ничего не делать). Чтобы так могло получиться вводятся параметры $\\gamma_k$ и $\\beta_k$ для масштабирования и сдвига нормализованной активации по каждой компоненте:\n",
    "$$y_k = \\gamma_k \\frac{x_k - E[x_k]}{\\sqrt{Var{[x_k]}}} + \\beta_k$$\n",
    "\n",
    "На практике, чтобы избежать деления на ноль при нормализации, к дисперсии добавляется небольшое эпсилон. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "mnist = input_data.read_data_sets(\"MNIST/\", one_hot=True)\n",
    "\n",
    "def fullyconnected_layer(tensor, input_size, out_size):\n",
    "    W = tf.Variable(tf.truncated_normal([input_size, out_size], stddev=0.1))\n",
    "    b = tf.Variable(tf.truncated_normal([out_size], stddev=0.1))\n",
    "    return tf.nn.tanh(tf.matmul(tensor, W) + b)\n",
    "\n",
    "def batchnorm_layer(tensor, size):\n",
    "    batch_mean, batch_var = tf.nn.moments(tensor, [0])\n",
    "    beta = tf.Variable(tf.zeros([size]))\n",
    "    scale = tf.Variable(tf.ones([size]))\n",
    "    eps = 0.001\n",
    "    return tf.nn.batch_normalization(tensor, batch_mean, batch_var, beta, scale, eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для эксперимента создадим полносвязную сеть с размерами слоев 784, 100, 100 и 10. Между промежуточными слоями мы и вставим слой нормализации по мини-батчам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "h1 = fullyconnected_layer(x, 784, 100)\n",
    "h1_bn = batchnorm_layer(h1, 100)\n",
    "h2 = fullyconnected_layer(h1_bn, 100, 100)\n",
    "y_logit = fullyconnected_layer(h2, 100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "#loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=y_logit))\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=y_logit, labels=y)\n",
    "train_op = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_logit, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "cost_list = []\n",
    "accuracy_list = []\n",
    "n = 2000\n",
    "for i in range(n):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    _, cost, accur = sess.run([train_op, loss, accuracy],\n",
    "                              feed_dict={x: batch_xs, y: batch_ys})        \n",
    "    #print(sess.run(y_logit, feed_dict={x: batch_xs}).shape)\n",
    "    \n",
    "    cost = sess.run(tf.reduce_mean(cost)) \n",
    "    #print(cost)\n",
    "    cost_list.append(cost)\n",
    "    accuracy_list.append(accur)\n",
    "\n",
    "ax1.plot(range(0, n), cost_list)\n",
    "ax1.set_title(\"Cost function\")\n",
    "ax2.plot(range(0, n), accuracy_list, c='r')\n",
    "\n",
    "ax2.set_title(\"Accuracy\")\n",
    "ax2.set_ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy:{0:.4f}\".format(sess.run(accuracy, feed_dict={x:mnist.test.images, y:mnist.test.labels})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И снова точность 97%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сверточная нейронная сеть для распознавания цифр\n",
    "\n",
    "Посмотрим какой результат нам даст сверточная конфигурация нейронной сети.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Входные данные для двумерной свертки в TensoFlow должны иметь четырехмерную структуру, которая выглядит так:\n",
    "$$[размер \\: батча, \\: высота, \\: ширина, \\: каналы]$$\n",
    "\n",
    "А размерность тензора сверточных весов определяется размерами ядра свертки и числом каналов как на входе, так и на выходе. Получается снова четырехмерный тензор, но уже следующего вида:\n",
    "$$[высота, \\: ширина, \\: входные \\: каналы, \\: выходные \\: каналы]$$\n",
    "\n",
    "Будем использовать 32 различных ядер свертки одного размера 5х5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "W_conv_1 = tf.Variable(tf.truncated_normal([5, 5, 1, 32], stddev=0.1))\n",
    "b_conv_1 = tf.Variable(tf.constant(0.1, shape=[32]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь $-1$ в `tf.reshape(x, [-1, 28, 28, 1])` означает заранее неизвестный размер мини-батча. А последняя единичка говорит нам о том, что канал всего лишь один, т.е. наши изображения монохромные. \n",
    "\n",
    "Теперь у нас определены переменные для всех весов сверточного слоя. Используем функцию `tf.nn.conv2d` для применения сверточных фильтров. Аргумент `strides` задает шаг по изображению. Зададим шаг 1 по всем размерностям. Аргумент `padding` говорит как быть с окнами, которые \"вылезают\" за границы входного массива."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_1 = tf.nn.conv2d(x_image, W_conv_1, strides=[1, 1, 1, 1], padding=\"SAME\") + b_conv_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве функции активации возьмем ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_conv_1 = tf.nn.relu(conv_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, слой фильтров с нелинейностью готов. Осталось только добавить слой субдискретизации (pooling). Обычно в качестве операции субдискретизации к каждой локальной группе нейронов применяется операция взятия максимума (max-pooling). \n",
    "\n",
    "Хотя в результате субдискретизации теряется часть информации, сеть становится более устойчивой к небольшим трансформациям изображения вроде сдвига и поворота. Параметр `ksize` задает размер окна субдискретизации по всем четыремя размерностям."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_pool_1 = tf.nn.max_pool(h_conv_1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметры `strides` и `padding`означают здесь то же самое, что в сверточном слое, только в этот раз мы двигаемся по изображению в обе стороны с шагом 2. Понятно, что после этого слоя размер изображения в обоиз направления уменьшится вдвое, до 14х14.\n",
    "\n",
    "Теперь давайте добавим еще один сверточный слой и слой субдискретизации, в этот раз используем на этом слое 64 фильтра:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv_2 = tf.Variable(tf.truncated_normal([5, 5, 32, 64], stddev=0.1))\n",
    "b_conv_2 = tf.Variable(tf.constant(0.1, shape=[64]))\n",
    "conv_2 = tf.nn.conv2d(h_pool_1, W_conv_2, strides=[1, 1, 1, 1], padding=\"SAME\") + b_conv_2\n",
    "\n",
    "h_conv_2 = tf.nn.relu(conv_2)\n",
    "h_pool_2 = tf.nn.max_pool(h_conv_2,  ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")#должно быть 7х7, но 14х14.Почему?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как правило, в глубоких нейронных сетях за сверточным слоем следуют полносвязные, задача которых состоит в том, чтобы \"собрать вместе\" все признаки из фильтров и собственно перевести их в самый последний слой, который выдаст ответ. Но для начала нам нужно из двумерного слоя сделать плоский:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_pool_2_flat = tf.reshape(h_pool_2, [-1, 7*7*64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Число $7 \\cdot  7 \\cdot 64$ возникло из-за того, что мы дважды применили субдискретизацию и при этом в последнем слое использовали 64 фильтра. И теперь осталось только добавить полносвязные слои. Добавляем первый слой из 1024 нейронов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_fc_1 = tf.Variable(tf.truncated_normal([7*7*64, 1024], stddev=0.1))\n",
    "b_fc_1 = tf.Variable(tf.constant(0.1, shape=[1024]))\n",
    "h_fc_1 = tf.nn.relu(tf.matmul(h_pool_2_flat, W_fc_1) + b_fc_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Регуляризуем его дропаутом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_propability = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "h_fc_1_drop = tf.nn.dropout(h_fc_1, keep_propability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавляем последний слой с десятью выходами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_fc_2 = tf.Variable(tf.truncated_normal([1024, 10], stddev=0.1))\n",
    "b_fc_2 = tf.Variable(tf.constant(0.1, shape=[10]))\n",
    "\n",
    "logit_conv = tf.matmul(h_fc_1_drop, W_fc_2) + b_fc_2\n",
    "y_conv = tf.nn.softmax(logit_conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось определить ошибку и ввести оптимизатор. Используем алгоритм адаптивного градиентонго спуска `Adam`. Его идея в том, что шаг изменения должен быть меньше у тех весов, котоыре в большей степени варьируются в данных, и больше у тех, которые менее изменчиввы в разных примерах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit_conv, labels=y))\n",
    "train_step = tf.train.AdamOptimizer(0.0001).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "for i in range(10000):\n",
    "    #rand_indexes = np.random.choice(range(X_train.shape[0]), batch_size)\n",
    "    #batch_xs, batch_ys = X_train[rand_indexes].reshape(batch_size, 784), Y_train[rand_indexes].reshape(batch_size, 10)\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(64)\n",
    "    sess.run(train_step, feed_dict = {x:batch_xs, y:batch_ys, keep_propability:0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#sess.run(accuracy, feed_dict = {x: mnist.test.images, y: mnist.test.labels, keep_propability: 1.} ) # так не работает из-за нехватки памяти на GPU\n",
    "error_rate_1 = sess.run(accuracy, feed_dict={x:mnist.test.images[:5000], y: mnist.test.labels[:5000], keep_propability: 1.})\n",
    "error_rate_2 = sess.run(accuracy, feed_dict={x:mnist.test.images[5000:], y: mnist.test.labels[5000:], keep_propability: 1.})\n",
    "error_rate = (error_rate_1 + error_rate_2)/2\n",
    "print(error_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#error_rate_1 = sess.run(accuracy, feed_dict = {x: X_test.reshape(10000, 784)[:5000], y: Y_test.reshape(10000, 10)[:5000], keep_propability: 1.})\n",
    "#error_rate_2 = sess.run(accuracy, feed_dict = {x: X_test.reshape(10000, 784)[5000:], y: Y_test.reshape(10000, 10)[5000:], keep_propability: 1.})\n",
    "#error_rate = (error_rate_1 + error_rate_2)/2\n",
    "#print(error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Точность улучшилась до 99%!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras\n",
    "\n",
    "Попробуем то же самое сделать в Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "#mnist = tf.keras.datasets.mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import MaxPooling2D, Conv2D\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "batch_size, img_rows, img_cols = 64, 28, 28\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "X_train = X_train.astype(\"float32\")\n",
    "X_test = X_test.astype(\"float32\")\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (5, 5),  input_shape=input_shape, padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"same\"))\n",
    "\n",
    "model.add(Conv2D(64, (5, 5), padding=\"same\", input_shape=input_shape))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"same\"))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train, batch_size=batch_size, epochs=10, verbose=1, validation_data=(X_test, Y_test))\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test loss score: %f\" % score[0])\n",
    "print(\"Test accuracy: %f\" % score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как и ожидалось, точность получилась сравнимая - примерно те же 99%. Однако заметим, что в процессе обучения модели достигались и более высокие результаты, однако мы не остановили процесс подгонки. Потому мы можем использовать другой подход, а именно остановить процесс обучения как только достигнем наилучшей точности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "model.fit(X_train, Y_train, \n",
    "          callbacks=[ModelCheckpoint(\"model.hdf5\", monitor=\"val_acc\",\n",
    "                                     save_best_only=True, save_weights_only=False, mode=\"auto\")],\n",
    "          validation_split=0.1, epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Test loss score: %f\" % score[0])\n",
    "print(\"Test accuracy: %f\" % score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итого, точность немного подросла - до 99.24%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
